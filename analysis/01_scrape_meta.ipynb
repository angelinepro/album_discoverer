{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 1: Get list of albums to draw recommendations from"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook scrapes a list of around 1000 critically acclaimed albums from Metacritic (or 11 pages of results), in order to get a list of albums to feed into the Spotify API (notebook 2). *This site has since been updated since I scraped it at the end of May 2020, so this code needs to be adapted to work again.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:44:20.929699Z",
     "start_time": "2020-06-15T01:44:19.995754Z"
    }
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n",
    "import time, os\n",
    "import pickle\n",
    "import sys\n",
    "\n",
    "sys.setrecursionlimit(1000000) #to allow pickling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scrape Metacritic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first 11 pages are cleanly scraped (and then pickled, stored in the data folder). This is ordered by critic ratings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:07:16.823429Z",
     "start_time": "2020-06-15T02:07:16.820436Z"
    }
   },
   "outputs": [],
   "source": [
    "#Start out by getting list of URLS to scrape.\n",
    "list_urls = []\n",
    "for i in range(0, 11):\n",
    "    full_url = 'https://www.metacritic.com/browse/albums/score/metascore/all/filtered' + '?page=' + str(i)\n",
    "    list_urls.append(full_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:44:26.947196Z",
     "start_time": "2020-06-15T01:44:26.943711Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_data(urls):\n",
    "    user_agent = {'User-agent': 'Mozilla/5.0'}\n",
    "    response = []\n",
    "    for i in urls:\n",
    "        response.append(requests.get(i, headers = user_agent))\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:07:34.763512Z",
     "start_time": "2020-06-15T02:07:33.341539Z"
    }
   },
   "outputs": [],
   "source": [
    "#Scrape URLs and save source.\n",
    "soups = get_data(list_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:45:55.096950Z",
     "start_time": "2020-06-15T01:45:55.093739Z"
    }
   },
   "outputs": [],
   "source": [
    "def make_soup(response_list):\n",
    "    soup = []\n",
    "    for i in response_list:\n",
    "        soup.append(BeautifulSoup(i.text, 'html5lib'))\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T02:07:50.860715Z",
     "start_time": "2020-06-15T02:07:46.763057Z"
    }
   },
   "outputs": [],
   "source": [
    "#Convert source into Beautiful Soup\n",
    "soup_source = make_soup(soups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pickle source data, to avoid needing to scrape the site again.\n",
    "# with open('data/meta_critic_source.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(soup_source, to_write)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract Album Information and Save as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:46:02.935263Z",
     "start_time": "2020-06-15T01:46:02.928344Z"
    }
   },
   "outputs": [],
   "source": [
    "# extracts album information from Beautiful Soup\n",
    "def clean_html(soup):\n",
    "    critic_rating = []\n",
    "    album_title = []\n",
    "    artist_name = []\n",
    "    user_rating = []\n",
    "    release_date = []\n",
    "    all_lists = [artist_name, user_rating, release_date]\n",
    "    for i in soup.findAll('div', {'class': 'basic_stat product_title'}):\n",
    "        album_title.append(i.text.strip())\n",
    "    for i in soup.findAll('div', {'class': 'metascore_w small release positive'}):\n",
    "        critic_rating.append(i.text)\n",
    "        for j,k in enumerate(i.findNext().findAll('span', {'class': 'data'})):\n",
    "            all_lists[j%len(all_lists)].append(k.text)\n",
    "    artist_name2 = all_lists[0]\n",
    "    user_rating2 = all_lists[1]\n",
    "    release_date2 = all_lists[2]\n",
    "\n",
    "    return pd.DataFrame(list(zip(album_title[:100], artist_name2, critic_rating, user_rating2, release_date2)),\n",
    "                       columns = ['Album_Title', 'Artist_Name', 'Critic_Rating', 'User_Rating', 'Release_Date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:46:08.268548Z",
     "start_time": "2020-06-15T01:46:08.264656Z"
    }
   },
   "outputs": [],
   "source": [
    "# combines information for each album into a dataframe\n",
    "def clean_page(soup_list):\n",
    "    df = pd.concat([clean_html(i) for i in soup_list])\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:46:09.722275Z",
     "start_time": "2020-06-15T01:46:09.118219Z"
    }
   },
   "outputs": [],
   "source": [
    "# get dataframe of all album information\n",
    "long_critics_df = clean_page(soup_source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:46:10.750958Z",
     "start_time": "2020-06-15T01:46:10.748035Z"
    }
   },
   "outputs": [],
   "source": [
    "critics_df = long_critics_df.reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-15T01:46:13.601931Z",
     "start_time": "2020-06-15T01:46:13.587331Z"
    }
   },
   "outputs": [],
   "source": [
    "critics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save album dataframe as pickle file. \n",
    "# with open('data/critics_df_all.pickle', 'wb') as to_write:\n",
    "#     pickle.dump(critics_df, to_write)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:metis] *",
   "language": "python",
   "name": "conda-env-metis-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
